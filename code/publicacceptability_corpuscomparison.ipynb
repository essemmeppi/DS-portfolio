{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5a1d8b-78d4-4953-8e4c-f0f168950f8e",
   "metadata": {},
   "source": [
    "# Comparing corpora media and parliamentary corpora with topic modeling\n",
    "\n",
    "Comparing corpora with topic modeling is a well-known open challenge. One of the central problems is the difference in size of different corpora. That's why we tried oversampling (parliamentary speeches) and undersampling (news media articles) corpora - but they could help introducing distortions in our datasets. \n",
    "\n",
    "Hence, we decided to perform exploration based on specific events. We consider those dates in which we have many speeches and around the same number of articles from the same period. This period is February 2023, when the French pension reform was introduced in Parliament. Then we perform a single NMF model and implement the corpora comparison techinique based on doc-topic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25abf65e-93ee-4121-b8b4-139aa8a263a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import string \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns; sns.set_style(\"whitegrid\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebc509-6698-4fbf-a0ba-394a83647c6b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f5e7e-7b2c-42fe-a4f2-94f2d2e641ec",
   "metadata": {},
   "source": [
    "### Parliament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af7e0f2-479a-416b-8612-d92417ee0207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1395, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data from session on retraite and speeches mentioning retraite\n",
    "df_title_retraite = pd.read_csv('/home/sparazzoli/lagrange-oecd/data/Parliament/parl_corpus/parl_title_retraite.csv')\n",
    "df_speech_retraite = pd.read_csv('/home/sparazzoli/lagrange-oecd/data/Parliament/parl_corpus/parl_speech_retraite.csv')\n",
    "\n",
    "# Merge data from session on retraite and speeches mentioning retraite\n",
    "df_parl = pd.concat([df_title_retraite, df_speech_retraite])\n",
    "df_parl = df_parl.dropna(subset='speech').drop_duplicates(subset='speech')\n",
    "\n",
    "# Filter speeches from November 2022 onwards\n",
    "df_parl['dateSeance'] = pd.to_datetime(df_parl['dateSeance'], format='%Y%m%d%H%M%S%f')\n",
    "df_parl = df_parl[df_parl['dateSeance'] >= pd.to_datetime('2022-11-01')]\n",
    "df_parl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a1a3c0-e620-42f4-8218-be2665b9c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "stopwords = set(STOP_WORDS)\n",
    "custom_sw_parl = {'réforme','retraite','retrait','oui','no','avis','mme','monsieur','parole','pouvoir','faire','devoir','bien',\n",
    "                  'falloir','prendre','aller','bon','vouloir','bien','favorable','sous','remettre'}\n",
    "stopwords.update(custom_sw_parl)\n",
    "\n",
    "def lemmatizer(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "\n",
    "def whitespace_remover(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token.strip() != '']\n",
    "    filtered_tokens_newline = [token for token in filtered_tokens if token != '\\n\\n']\n",
    "    return filtered_tokens_newline\n",
    "    \n",
    "def sw_remover(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "def punct_remover(tokens):\n",
    "    filtered_tokens = [token for token in tokens if not all(char in string.punctuation for char in token)]\n",
    "    return filtered_tokens\n",
    "\n",
    "def joiner(tokens):\n",
    "    joined_tokens = ' '.join(tokens)\n",
    "    return joined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee07687-3c90-42d5-9d1b-31f90c79cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████▎           | 989/1395 [00:32<00:11, 34.33it/s]"
     ]
    }
   ],
   "source": [
    "# Create a new column 'TextLemmatized'\n",
    "df_parl['TextLemmatized'] = df_parl['speech'].progress_apply(lemmatizer)\n",
    "df_parl['TextProcessed'] = df_parl['TextLemmatized'].progress_apply(whitespace_remover)\n",
    "df_parl['TextProcessed'] = df_parl['TextProcessed'].progress_apply(sw_remover)\n",
    "df_parl['TextProcessed'] = df_parl['TextProcessed'].progress_apply(punct_remover)\n",
    "df_parl['TextProcessed'] = df_parl['TextProcessed'].progress_apply(joiner)\n",
    "\n",
    "df_parl['TextProcessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca882fc-6f98-413d-87ba-b24b11c6773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parl.reset_index(inplace=True)\n",
    "\n",
    "# Convert the 'dateSeance' column to a datetime data type if not already done\n",
    "df_parl['dateSeance'] = pd.to_datetime(df_parl['dateSeance'])\n",
    "\n",
    "# Set 'dateSeance' as the index\n",
    "df_parl.set_index('dateSeance', inplace=True)\n",
    "\n",
    "# Group by Day and count speeches\n",
    "df_grouped = df_parl.resample('D').size()\n",
    "\n",
    "# Reindex the DataFrame with a daily frequency and fill missing values with 0\n",
    "df_grouped = df_grouped.reindex(pd.date_range(df_grouped.index.min(), df_grouped.index.max(), freq='D'), fill_value=0)\n",
    "\n",
    "# Set up a larger figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot time series with rolling average\n",
    "plt.plot(df_grouped.index.astype(str), df_grouped.rolling(window=3, min_periods=1).mean(), label='Number of Speeches', color='dodgerblue')\n",
    "plt.ylabel('Number of Speeches')\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(df_grouped.index.astype(str)[::7], rotation=45)\n",
    "\n",
    "for date in ['2023-05-03', '2023-02-27', '2023-03-01', '2023-06-08']:\n",
    "   plt.axvline(x=date, color='coral', linestyle=':', zorder=0)\n",
    "\n",
    "plt.legend(labels=['Daily number of speeches', \"Sessions about the réforme\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c652101-d562-40e7-a48b-8877761cb1d9",
   "metadata": {},
   "source": [
    "### Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2354b88-dfc9-4b41-ab5a-d52b8d4c4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df_selectedmedia = pd.read_csv('/home/sparazzoli/lagrange-oecd/data/GDELT_221001_selectedmedia_Dec23.csv')\n",
    "df_selectedmedia['DATE'] = pd.to_datetime(df_selectedmedia['DATE'])\n",
    "df_selectedmedia.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc612e-cdfa-4d68-bd57-ccc0d04c1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_filt = df_selectedmedia[(df_selectedmedia['DATE'] > pd.to_datetime('2023-02-01')) & (df_selectedmedia['DATE'] < pd.to_datetime('2023-03-02'))]\n",
    "df_selected_filt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd9e194-3922-4685-8624-3c3fb1ead63e",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a28299-c980-40ac-97a6-48daba552722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parl.reset_index(inplace=True)\n",
    "\n",
    "df_parl_filtered = df_parl[(df_parl['dateSeance'] > pd.to_datetime('2023-02-01')) & (df_parl['dateSeance'] < pd.to_datetime('2023-03-02'))]\n",
    "df_parl_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a4c3e-276d-42f6-a9a7-93cc5382a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parl_text = df_parl_filtered[['dateSeance','TextProcessed','speech']]\n",
    "df_parl_text.rename(columns={\"dateSeance\": \"Date\", \"TextProcessed\": \"TextProcessed\", \"speech\":\"Text\"}, inplace = True)\n",
    "df_parl_text['Source'] = 'Parliament'\n",
    "df_parl_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50654038-98a6-4182-9634-e60d1ffecf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selectedmedia_text = df_selected_filt[['DATE','TextProcessed','Text']]\n",
    "df_selectedmedia_text.rename(columns={\"DATE\": \"Date\", \"TextProcessed\": \"TextProcessed\",'Text':'Text'}, inplace = True)\n",
    "df_selectedmedia_text['Source'] = 'Media'\n",
    "df_selectedmedia_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665a88c-9a6e-4d18-baf0-db98f9f9cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.concat([df_parl_text, df_selectedmedia_text])\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e602967-25e8-49c0-868d-c08e946d2149",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f742c9-edc5-4960-958a-f734698cdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df_merge['TextProcessed'].to_list()\n",
    "documents = [doc.lower() for doc in documents]\n",
    "print('The number of documents is: '+str(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ad03a-b64e-4084-bf01-0861a23630db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables which will be used later\n",
    "n_features = 5000\n",
    "n_components = 40 \n",
    "n_top_words = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5e652-608e-4362-abdc-6d4fb59e72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=5,  \n",
    "    max_features=n_features, \n",
    "    ngram_range=(1, 1))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062edfe2-51a9-4332-8171-b7e99ba1f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform NMF\n",
    "seed = 30\n",
    "nmf = NMF(n_components=n_components, \n",
    "          random_state=seed, # Pass an int for reproducible results across multiple function calls.\n",
    "          alpha_W=0, # Constant that multiplies the regularization terms of W. Set it to zero (default) to have no regularization on W\n",
    "          l1_ratio=0, # The regularization mixing parameter 0-1. l1_ratio = 0, penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n",
    "          max_iter=1000,\n",
    "          init='random'\n",
    "          ).fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d244a9-de4c-4918-b78a-6c00b08ee0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title, name, n_topics, n_seed):\n",
    "    fig, axes = plt.subplots(5, 8, figsize=(40, 30), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=50)\n",
    "\n",
    "    plt.subplots_adjust(top=0.93, bottom=0.05, wspace=0.7, hspace=0.2)\n",
    "    #plt.savefig('./figures/'+str(name)+'_topics'+str(n_topics)+'_seed'+str(n_seed)+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472357c0-2f44-4372-b57a-7267a085cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "#plot_top_words(\n",
    "#    nmf, tfidf_feature_names, n_top_words, \"Topics in merged documents (NMF)\", 'scikit_custParlSW_5kFeat', n_components, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e046d-7187-4488-b841-625808b73238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names from the vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the top 3 indices for each topic\n",
    "top_indices = nmf.components_.argsort()[:,-5:]\n",
    "\n",
    "# Create empty list to store top words  \n",
    "top_words = []\n",
    "\n",
    "# Loop through each topic\n",
    "for topic in top_indices:\n",
    "  \n",
    "  # Extract top 3 words\n",
    "  top_3 = [feature_names[i] for i in topic]\n",
    "  \n",
    "  # Add to list\n",
    "  top_words.extend(top_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26189a2c-b5e4-47c6-85ce-c2e1d0a39ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document-topic matrix\n",
    "doc_topic_matrix = nmf.transform(tfidf_matrix)\n",
    "\n",
    "# Calculate topic frequencies across corpus\n",
    "topic_freqs = doc_topic_matrix.sum(axis=0) \n",
    "\n",
    "# Get top 3 words for each topic\n",
    "top_word_indices = nmf.components_.argsort()[:, -3:]\n",
    "top_words = [[tfidf_feature_names[i] for i in topic[::-1]] for topic in top_word_indices]\n",
    "top_words = [' '.join(words) for words in top_words]\n",
    "\n",
    "# Combine topics and top words\n",
    "topics_with_words = list(zip(topic_freqs, top_words))\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_topics = sorted(topics_with_words, key=lambda x: x[0], reverse=False)\n",
    "\n",
    "# Extract sorted freqs and topic names  \n",
    "sorted_freqs, sorted_top_words = zip(*sorted_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd0cd6-6ed0-4cd4-b8d2-4630b150e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sorted topics and freqs  \n",
    "fig, ax = plt.subplots(figsize=(4, 8)) \n",
    "\n",
    "# Plot horizontal bars\n",
    "ax.barh(range(n_components), sorted_freqs, color='dodgerblue')\n",
    "\n",
    "# Set y-ticks and labels\n",
    "ax.set_yticks(range(n_components)) \n",
    "ax.set_yticklabels(sorted_top_words)\n",
    "\n",
    "# Remove x-axis tick labels  \n",
    "ax.set_xticks([]) \n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel(\"Frequency\")\n",
    "\n",
    "# Title and tight layout\n",
    "ax.set_title(f\"Topics in merged docs\\n(N = {len(doc_topic_matrix)})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig(os.path.join('/home/sparazzoli/lagrange-oecd/code/corpus_comparison/undersampling_plots', f'{timestamp_str}_topicdistribution.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7742d-4c21-409b-9d96-955b683cbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix_with_source = np.column_stack((doc_topic_matrix, df_merge[\"Source\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcda72-97bd-4594-a692-d8944ad6e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_vector = np.mean(doc_topic_matrix_with_source[df_merge[\"Source\"] == \"Media\"][:, :-1], axis=0)\n",
    "parliament_vector = np.mean(doc_topic_matrix_with_source[df_merge[\"Source\"] == \"Parliament\"][:, :-1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d77988-70c4-4a8f-8ae7-771a726c2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top three words for each topic\n",
    "top_word_indices = nmf.components_.argsort()[:, -3:]\n",
    "top_words = [[tfidf_feature_names[i] for i in topic[::-1]] for topic in top_word_indices]\n",
    "top_words = [' '.join(words) for words in top_words]\n",
    "\n",
    "# Set the column names while creating the DataFrame\n",
    "df_corporacomparison = pd.DataFrame([media_vector, parliament_vector],columns=top_words, index=['Media', 'Parliament'])\n",
    "\n",
    "df_corporacomparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b73ae1-6002-4391-bf06-044df6e39814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the DataFrame to have topics as rows and corpora as columns\n",
    "df_corpcomp_T = df_corporacomparison.transpose()\n",
    "\n",
    "# Calculate the difference between Media and Parliament for each topic\n",
    "df_corpcomp_T['Difference'] = (df_corpcomp_T['Media'] - df_corpcomp_T['Parliament'])*100/(df_corpcomp_T['Media'].max() + df_corpcomp_T['Parliament'].max())\n",
    "\n",
    "# Sort the DataFrame based on the 'Difference' column\n",
    "df_corpcomp_T_sorted = df_corpcomp_T.sort_values(by='Difference', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6624b-c03c-4767-bc23-c7fee094683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig = plt.figure(figsize=(8, 10)) \n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Normalize the 'Difference' values for color assignment\n",
    "norm = plt.Normalize(vmin=df_corpcomp_T_sorted['Difference'].min(), vmax=df_corpcomp_T_sorted['Difference'].max())\n",
    "\n",
    "# Plot the bars with color based on the 'Difference' values using the 'RdYlBu' colormap\n",
    "bars = ax.barh(df_corpcomp_T_sorted.index, df_corpcomp_T_sorted['Difference'], color=plt.cm.RdBu(norm(df_corpcomp_T_sorted['Difference'])))\n",
    "\n",
    "# Set the labels and title\n",
    "plt.xlabel('Topic Coverage Mismatch (%)')\n",
    "plt.ylabel('Topics')\n",
    "plt.title(f'Topic Coverage Difference b/w Media (N = {len(df_selectedmedia_text)})\\nand Parliament (N = {len(df_parl_text)}) in February 2023')\n",
    "\n",
    "# Add light grey grid\n",
    "ax.grid(axis='x', linestyle=':', alpha=0.5, color='lightgrey', zorder=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.set_size_inches(8, 10, forward=True)\n",
    "#fig.savefig('/home/sparazzoli/lagrange-oecd/code/corpus_comparison/TopicCoverageDifference_February.png', dpi=300, bbox_inches='tight') # Set desired DPI value and pass 'bbox_inches' argument to remove white spaces around the edges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
